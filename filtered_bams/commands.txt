# command log and scripts to take raw reads, map to reference, and filter to produce filtered bam files

# I will follow the same process from .fq raw reads to .bam mappped reads using bowtie2
# and filtering (e.g. of duplicates) using picard tools as is described in bioinfo_pipe.txt
# for the reference panel bee genomes (and earlier CA bees) downloaded as .fq files from NCBI

# First sequencing run from Novogene completed 12/12/18, dowloaded from ftp using FileZilla (interactive) 12/18/18
# and saved in data/novo_seq/C202SC18101772/. Backups saved to linux on Willowbank and Box.com on cloud.
# Check MD5 that everything downloaded properly (all good!):
# bees/data/novo_seq/C202SC18101772/raw_data$ cat */MD5.txt > ALL_MD5_expected.txt # expected md5 values
# raw_data$ for i in $(awk '{print $2}' ALL_MD5_expected.txt); do md5 */${i} | awk '{print $4}'; done > ALL_MD5_observed.txt # observed md5 values for each .fq file
# raw_data$ diff -q <(awk '{print $1}' ALL_MD5_expected.txt) <(cat ALL_md5_observed.txt) # outputs 'Files * and * differ' if they differ; outputs nothing if they're the same. Take away -q to see differences.

# Re-downloaded honeybee genome Apis mellifera v4.5 from beebase.org on 12.19.18: data/honeybee_genome/Amel_4.5_scaffolds.fa (note: wasn't .fa.gz)

# Also downloaded Bowtie2 for macs: v2.3.3.1 to match what I used on the Linux machine previously. Downloaded 12.19.18
# re-indexed v4.5 genome from www.beebase.org (it's not gzipped the genome I downloaded)
bees/data/honeybee_genome$ bowtie2-build Amel_4.5_scaffolds.fa honeybee_Amel_4.5

# made a short script that maps reads for new bees just sequenced: map_reads.sh
# to run alignment in bulk, I first made a list of sample IDs sequenced in this batch C202SC18101772:
data/novo_seq$ awk '{print $5}' seq1_novogene_index_i7_bee_id_link.txt | tail -n +2 > C202SC18101772/samples.list
scripts$ nohup parallel --noswap --joblog ../logs/map_reads_C202SC18101772.log --jobs 4 \
./map_reads.sh {1} C202SC18101772 :::: ../data/novo_seq/C202SC18101772/samples.list &> ../logs/map_reads_C202SC18101772.out &
[1] 72328 - RUNNING 12.19.18 - DONE (some cancelled because already completed on other computer)
data/novo_seq/C202SC18101772$ tac samples.list > samples_reversed_order.list
scripts$ nohup parallel --noswap --joblog ../logs/map_reads_C202SC18101772_reversed_order.log --jobs 8 \
./map_reads.sh {1} C202SC18101772 :::: ../data/novo_seq/C202SC18101772/samples_reversed_order.list &> ../logs/map_reads_C202SC18101772_reversed_order.out &
[1] 7754 - RUNNING 12.19.18 on Barbara - cancelled due to memory. Now rerunning:
[didn't print new number/overwritten by nano, but it's running] - DONE; I cancelled last few when every bam was finished on one computer. Moved bams to Nancy's comp.
# some bams were completed by both computers and I noticed very small discrepancies in # lines per file (diff. of ~5-30 lines out of thousands)
# bowtie2 is the same version 2.3.3.1 and random seed is the same (2014) so this must be a small remaining stochasticity affected by compiling the program on mac vs. Ubuntu.
# for those bams with duplicated efforts I used the ones run on the mac going forward.
# Now filtering bams with new scripts sort_bams.sh and dedup_baq_bams.sh
scripts$ nohup parallel --noswap --joblog ../logs/filter_bams_C202SC18101772.log --jobs 4 \
'./sort_bams.sh {1} novo_seq/bam_files; ./dedup_baq_bams.sh {1} filtered_bams/results' \
:::: ../data/novo_seq/C202SC18101772/samples.list &> ../logs/filter_bams_C202SC18101772.out &
[1] 27043 -- typo, try again:
[1] 29784 -- needed to source ~/.profile to get variable $PICARD
[1] 31078 -- failed due to typo in file naming
[2] 31767 -RUNNING 12.20.18 19:15

# Now I am re-mapping all reference samples from Harpur 2014:
Harpur_2014_NCBI$ cat bam_files_old/IDs.list | cut -d'_' -f2 > samples.list
scripts$ chmod u+x map_reads_single.sh
scripts$ nohup parallel --noswap --joblog ../logs/map_reads_Harpur_2014_NCBI.log --jobs 4 \
./map_reads_single.sh {1} Harpur_2014_NCBI :::: ../data/Harpur_2014_NCBI/samples.list \
&> ../logs/map_reads_Harpur_2014_NCBI.out &
[1] 32424 - RUNNING 12.20.18 10:30. Reran ones that didn't go:
First 4 ran without issues, but then I moved the mapping script and they failed, so rerunning those:
filtered_bams$ nohup parallel --noswap --joblog ../logs/map_reads_Harpur_2014_NCBI_2.log --jobs 4 ./map_reads_single.sh {1} Harpur_2014_NCBI \
::: $(tail -n +5 ../data/Harpur_2014_NCBI/samples.list) &> ../logs/map_reads_Harpur_2014_NCBI_2.out &
[1] 14980 - RUNNING 12.21.18 # COMPLETED ALL DONE

# run sort_bams.sh and dedup_baq_bams.sh on 'original' bams from CA_Bees
# (note inconsistent in CA_Bee/bam_from_Julie naming with *.merged.bam or *.combined.bam)
# first made symlinks to bams for consistent naming on CA_Bee
CA_Bee/bam_files$ for i in $(cat ../samples.list); do ln -s ../bam_from_Julie/$i.*.bam $i.bam; done
# then run filtering steps on CA_Bee bams
bees/filtered_bams$ nohup parallel --noswap --joblog ../logs/filter_bams_CA_Bee.log --jobs 4 './sort_bams.sh {1} ../data/CA_Bee/bam_files; ./dedup_baq_bams.sh {1} filtered_bams/results' :::: ../data/CA_Bee/samples.list &> ../logs/filter_bams_CA_Bee.out &
[1] 6017 -- fixed some typos in directory, memory and PICARD variable
# make logs directory, then run:
bees/filtered_bams$ mkdir logs
bees/filtered_bams$ nohup parallel --noswap --joblog logs/filter_bams_CA_Bee.log --jobs 4 './sort_bams.sh {1} ../data/CA_Bee/bam_files; ./dedup_baq_bams.sh {1} results' :::: ../data/CA_Bee/samples.list &> logs/filter_bams_CA_Bee.out &
[1] 28013 - Low memory & wrong path to reference genome. Maxed all memory to 6G and trying again with 3 threads:
bees/filtered_bams$ nohup parallel --noswap --joblog logs/filter_bams_CA_Bee.log --jobs 3 './sort_bams.sh {1} ../data/CA_Bee/bam_files; ./dedup_baq_bams.sh {1} results' :::: ../data/CA_Bee/samples.list &> logs/filter_bams_CA_Bee.out &
[1] 31247 - RUNNING SUCCESSFULLY 12.27.18 -- ap29, ap30 and ap31 will need to be rerun (modified script while running for general case when input isn't from results/ folder)
bees/filtered_bams$ nohup bash -c "until (( $(wc -l < logs/dedup_bams_Kenya_Sheppard_NCBI.log) == 7 )); do sleep 100; done; echo "done waiting for dedup_bams_Kenya_Sheppard_NCBI to finish. Now running bam filtering for 3 CA_Bees that failed previously:"; parallel --noswap --joblog logs/filter_bams_CA_Bee_2.log --jobs 2 'echo "filtering bam for bee: " {1}; ./sort_bams.sh {1} ../data/CA_Bee/bam_files; ./dedup_baq_bams.sh {1} results' :::: ap29 ap30 ap31 &> logs/filter_bams_CA_Bee_2.out" &
[3] 862 - WAITING on other file's output - KILLED (low memory; other job didn't finish)
RERUNNING THOSE THAT CRASHED FROM LACK OF MEMORY; removing intermediate file .sort.bam as each one completes deduping:
bees/filtered_bams$ nohup parallel --noswap --joblog logs/filter_bams_CA_Bee_rerun2.log --jobs 2 './sort_bams.sh {1} ../data/CA_Bee/bam_files; ./dedup_baq_bams.sh {1} results; rm results/{1}.sort.bam' :::: logs/CA_Bee_samples_rerun2.list &> logs/filter_bams_CA_Bee_rerun2.out &
[1] 10206 - COMPLETE 1.2.19
# add readgroups RG to CA bees:
bees/filtered_bams$ nohup parallel --noswap --jobs 4 --joblog logs/add_RG_CA_Bee.log \
'add_readgroup_2_bams.sh {1} CA_Bee results :::: ../data/CA_Bee/samples.list \
&> logs/add_RG_CA_Bee.out &

# run dedup_baq_bams.sh on Kenyan bees --> also put in folder filtered_bams/results
# note: these have already been mapped and sorted, just need to be deduped (I did not delete intermediate sorted file from last time)
bees/data/Kenya_Sheppard_NCBI/bam_files$ cat IDs.list | cut -d'_' -f2 > ../samples.list
bees/filtered_bams$ nohup parallel --noswap --joblog logs/dedup_bams_Kenya_Sheppard_NCBI.log --jobs 2 './dedup_baq_bams.sh {1} ../data/Kenya_Sheppard_NCBI/bam_files' :::: ../data/Kenya_Sheppard_NCBI/samples.list &> logs/dedup_bams_Kenya_Sheppard_NCBI.out &
[1] 32097 - fixed typo messing up runs where dir in is not results/:
[1] 32303 - COMPLETED 12.27.18. I had to delete intermediate sorted bams for space.

# run filtering steps on Harpur bees --> put all in one consistent folder in filtered_bams/results
bees/filtered_bams$ nohup bash -c "until (( $(wc -l < logs/filter_bams_CA_Bee.log) == 76 )); do sleep 100; done; echo "done waiting for filter_bams_CA_Bee to finish. Starting Harpur_2014_NCBI bam filtering:"; parallel --noswap --joblog logs/filter_bams_Harpur_2014_NCBI.log --jobs 5 --delay 3 'echo "now filtering bam for bee ID: " {1}; ./sort_bams.sh {1} ../data/Harpur_2014_NCBI/bam_files; ./dedup_baq_bams.sh {1} results' :::: ../data/Harpur_2014_NCBI/samples.list &> logs/filter_bams_Harpur_2014_NCBI.out" &
[2] 854 - WAITING on other file's output - KILLED (low memory; other job didn't finish) .. just running 1 job at a time:
bees/filtered_bams$ nohup parallel --noswap --joblog logs/filter_bams_Harpur_2014_NCBI.log --jobs 1 './sort_bams.sh {1} ../data/Harpur_2014_NCBI/bam_files; ./dedup_baq_bams.sh {1} results; rm results/{1}.sort.bam' :::: ../data/Harpur_2014_NCBI/samples.list &> logs/filter_bams_Harpur_2014_NCBI.out &
[2] 11441 - COMPLETE 1.2.19
# would run more in parallel except one other job w/ 2 threads is running at the same time and $ free -m says I have < 6G RAM available, which is what I've maxed picard and samtools to.


# TO DO:
# add readgroup IDs to CA_Bees filtered bams ..others already have this from when I mapped with bowtie2
# delete intermediate bams if you need the space (likely).
# note: best to put more general scripts, that will be used in multiple analyses, when that comes up, in a scripts/ folder
# also need a script to calculate coverage distribution and mean coverage (to get 2x and .5x thresholds)
