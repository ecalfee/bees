# command log and scripts to take raw reads, map to reference, and filter to produce filtered bam files

# I will follow the same process from .fq raw reads to .bam mappped reads using bowtie2
# and filtering (e.g. of duplicates) using picard tools as is described in bioinfo_pipe.txt
# for the reference panel bee genomes (and earlier CA bees) downloaded as .fq files from NCBI

# First sequencing run from Novogene completed 12/12/18, dowloaded from ftp using FileZilla (interactive) 12/18/18
# and saved in data/novo_seq/C202SC18101772/. Backups saved to linux on Willowbank and Box.com on cloud.
# Check MD5 that everything downloaded properly (all good!):
# bees/data/novo_seq/C202SC18101772/raw_data$ cat */MD5.txt > ALL_MD5_expected.txt # expected md5 values
# raw_data$ for i in $(awk '{print $2}' ALL_MD5_expected.txt); do md5 */${i} | awk '{print $4}'; done > ALL_MD5_observed.txt # observed md5 values for each .fq file
# raw_data$ diff -q <(awk '{print $1}' ALL_MD5_expected.txt) <(cat ALL_md5_observed.txt) # outputs 'Files * and * differ' if they differ; outputs nothing if they're the same. Take away -q to see differences.
# checking data on barbara:
# raw_data$ for i in $(awk '{print $2}' ALL_MD5_expected.txt); do md5sum */${i}; done > ALL_MD5_observed_barbara_1.18.19.txt # observed md5 values for each .fq file
# check ALL_MD5_observed_barbara_1.18.19.txt against expected checksums:
# raw_data$ diff -q <(awk '{print $1}' ALL_MD5_expected.txt) <(awk '{print $1}' ALL_MD5_observed_barbara_1.18.19.txt) # outputs 'Files * and * differ' if they differ; outputs nothing if they're the same. Take away -q to see differences. # ALL GOOD!


# Re-downloaded honeybee genome Apis mellifera v4.5 from beebase.org on 12.19.18: data/honeybee_genome/Amel_4.5_scaffolds.fa (note: wasn't .fa.gz)

# Also downloaded Bowtie2 for macs: v2.3.3.1 to match what I used on the Linux machine previously. Downloaded 12.19.18
# re-indexed v4.5 genome from www.beebase.org (it's not gzipped the genome I downloaded)
bees/data/honeybee_genome$ bowtie2-build Amel_4.5_scaffolds.fa honeybee_Amel_4.5

# made a short script that maps reads for new bees just sequenced: map_reads.sh
# to run alignment in bulk, I first made a list of sample IDs sequenced in this batch C202SC18101772:
data/novo_seq$ awk '{print $5}' seq1_novogene_index_i7_bee_id_link.txt | tail -n +2 > C202SC18101772/samples.list
scripts$ nohup parallel --noswap --joblog ../logs/map_reads_C202SC18101772.log --jobs 4 \
./map_reads.sh {1} C202SC18101772 :::: ../data/novo_seq/C202SC18101772/samples.list &> ../logs/map_reads_C202SC18101772.out &
[1] 72328 - RUNNING 12.19.18 - DONE (some cancelled because already completed on other computer)
data/novo_seq/C202SC18101772$ tac samples.list > samples_reversed_order.list
scripts$ nohup parallel --noswap --joblog ../logs/map_reads_C202SC18101772_reversed_order.log --jobs 8 \
./map_reads.sh {1} C202SC18101772 :::: ../data/novo_seq/C202SC18101772/samples_reversed_order.list &> ../logs/map_reads_C202SC18101772_reversed_order.out &
[1] 7754 - RUNNING 12.19.18 on Barbara - cancelled due to memory. Now rerunning:
[didn't print new number/overwritten by nano, but it's running] - DONE; I cancelled last few when every bam was finished on one computer. Moved bams to Nancy's comp.
# some bams were completed by both computers and I noticed very small discrepancies in # lines per file (diff. of ~5-30 lines out of thousands)
# bowtie2 is the same version 2.3.3.1 and random seed is the same (2014) so this must be a small remaining stochasticity affected by compiling the program on mac vs. Ubuntu.
# for those bams with duplicated efforts I used the ones run on the mac going forward.
# Now filtering bams with new scripts sort_bams.sh and dedup_baq_bams.sh
scripts$ nohup parallel --noswap --joblog ../logs/filter_bams_C202SC18101772.log --jobs 4 \
'./sort_bams.sh {1} novo_seq/bam_files; ./dedup_baq_bams.sh {1} filtered_bams/results' \
:::: ../data/novo_seq/C202SC18101772/samples.list &> ../logs/filter_bams_C202SC18101772.out &
[1] 27043 -- typo, try again:
[1] 29784 -- needed to source ~/.profile to get variable $PICARD
[1] 31078 -- failed due to typo in file naming
[2] 31767 -RUNNING 12.20.18 19:15

# Now I am re-mapping all reference samples from Harpur 2014:
Harpur_2014_NCBI$ cat bam_files_old/IDs.list | cut -d'_' -f2 > samples.list
scripts$ chmod u+x map_reads_single.sh
scripts$ nohup parallel --noswap --joblog ../logs/map_reads_Harpur_2014_NCBI.log --jobs 4 \
./map_reads_single.sh {1} Harpur_2014_NCBI :::: ../data/Harpur_2014_NCBI/samples.list \
&> ../logs/map_reads_Harpur_2014_NCBI.out &
[1] 32424 - RUNNING 12.20.18 10:30. Reran ones that didn't go:
First 4 ran without issues, but then I moved the mapping script and they failed, so rerunning those:
filtered_bams$ nohup parallel --noswap --joblog ../logs/map_reads_Harpur_2014_NCBI_2.log --jobs 4 ./map_reads_single.sh {1} Harpur_2014_NCBI \
::: $(tail -n +5 ../data/Harpur_2014_NCBI/samples.list) &> ../logs/map_reads_Harpur_2014_NCBI_2.out &
[1] 14980 - RUNNING 12.21.18 # COMPLETED ALL DONE

# run sort_bams.sh and dedup_baq_bams.sh on 'original' bams from CA_Bees
# (note inconsistent in CA_Bee/bam_from_Julie naming with *.merged.bam or *.combined.bam)
# first made symlinks to bams for consistent naming on CA_Bee
CA_Bee/bam_files$ for i in $(cat ../samples.list); do ln -s ../bam_from_Julie/$i.*.bam $i.bam; done
# then run filtering steps on CA_Bee bams
bees/filtered_bams$ nohup parallel --noswap --joblog ../logs/filter_bams_CA_Bee.log --jobs 4 './sort_bams.sh {1} ../data/CA_Bee/bam_files; ./dedup_baq_bams.sh {1} filtered_bams/results' :::: ../data/CA_Bee/samples.list &> ../logs/filter_bams_CA_Bee.out &
[1] 6017 -- fixed some typos in directory, memory and PICARD variable
# make logs directory, then run:
bees/filtered_bams$ mkdir logs
bees/filtered_bams$ nohup parallel --noswap --joblog logs/filter_bams_CA_Bee.log --jobs 4 './sort_bams.sh {1} ../data/CA_Bee/bam_files; ./dedup_baq_bams.sh {1} results' :::: ../data/CA_Bee/samples.list &> logs/filter_bams_CA_Bee.out &
[1] 28013 - Low memory & wrong path to reference genome. Maxed all memory to 6G and trying again with 3 threads:
bees/filtered_bams$ nohup parallel --noswap --joblog logs/filter_bams_CA_Bee.log --jobs 3 './sort_bams.sh {1} ../data/CA_Bee/bam_files; ./dedup_baq_bams.sh {1} results' :::: ../data/CA_Bee/samples.list &> logs/filter_bams_CA_Bee.out &
[1] 31247 - RUNNING SUCCESSFULLY 12.27.18 -- ap29, ap30 and ap31 will need to be rerun (modified script while running for general case when input isn't from results/ folder)
bees/filtered_bams$ nohup bash -c "until (( $(wc -l < logs/dedup_bams_Kenya_Sheppard_NCBI.log) == 7 )); do sleep 100; done; echo "done waiting for dedup_bams_Kenya_Sheppard_NCBI to finish. Now running bam filtering for 3 CA_Bees that failed previously:"; parallel --noswap --joblog logs/filter_bams_CA_Bee_2.log --jobs 2 'echo "filtering bam for bee: " {1}; ./sort_bams.sh {1} ../data/CA_Bee/bam_files; ./dedup_baq_bams.sh {1} results' :::: ap29 ap30 ap31 &> logs/filter_bams_CA_Bee_2.out" &
[3] 862 - WAITING on other file's output - KILLED (low memory; other job didn't finish)
RERUNNING THOSE THAT CRASHED FROM LACK OF MEMORY; removing intermediate file .sort.bam as each one completes deduping:
bees/filtered_bams$ nohup parallel --noswap --joblog logs/filter_bams_CA_Bee_rerun2.log --jobs 2 './sort_bams.sh {1} ../data/CA_Bee/bam_files; ./dedup_baq_bams.sh {1} results; rm results/{1}.sort.bam' :::: logs/CA_Bee_samples_rerun2.list &> logs/filter_bams_CA_Bee_rerun2.out &
[1] 10206 - COMPLETE 1.2.19
# add readgroup IDs to CA_Bees filtered bams ..others already have this from when I mapped with bowtie2
bees/filtered_bams$ nohup parallel --noswap --delay 3 --jobs 4 --joblog logs/add_RG_CA_Bee.log \
'./add_readgroup_2_bams.sh {1} CA_Bee results' :::: ../data/CA_Bee/samples.list \
&> logs/add_RG_CA_Bee.out &
[2] 14531 - COMPLETE 1.2.19 (note: deleted I intermediate bams for space)

# run dedup_baq_bams.sh on Kenyan bees --> also put in folder filtered_bams/results
# note: these have already been mapped and sorted, just need to be deduped (I did not delete intermediate sorted file from last time)
bees/data/Kenya_Sheppard_NCBI/bam_files$ cat IDs.list | cut -d'_' -f2 > ../samples.list
bees/filtered_bams$ nohup parallel --noswap --joblog logs/dedup_bams_Kenya_Sheppard_NCBI.log --jobs 2 './dedup_baq_bams.sh {1} ../data/Kenya_Sheppard_NCBI/bam_files' :::: ../data/Kenya_Sheppard_NCBI/samples.list &> logs/dedup_bams_Kenya_Sheppard_NCBI.out &
[1] 32097 - fixed typo messing up runs where dir in is not results/:
[1] 32303 - COMPLETED 12.27.18. I had to delete intermediate sorted bams for space.

# run filtering steps on Harpur bees --> put all in one consistent folder in filtered_bams/results
bees/filtered_bams$ nohup bash -c "until (( $(wc -l < logs/filter_bams_CA_Bee.log) == 76 )); do sleep 100; done; echo "done waiting for filter_bams_CA_Bee to finish. Starting Harpur_2014_NCBI bam filtering:"; parallel --noswap --joblog logs/filter_bams_Harpur_2014_NCBI.log --jobs 5 --delay 3 'echo "now filtering bam for bee ID: " {1}; ./sort_bams.sh {1} ../data/Harpur_2014_NCBI/bam_files; ./dedup_baq_bams.sh {1} results' :::: ../data/Harpur_2014_NCBI/samples.list &> logs/filter_bams_Harpur_2014_NCBI.out" &
[2] 854 - WAITING on other file's output - KILLED (low memory; other job didn't finish) .. just running 1 job at a time:
bees/filtered_bams$ nohup parallel --noswap --joblog logs/filter_bams_Harpur_2014_NCBI.log --jobs 1 './sort_bams.sh {1} ../data/Harpur_2014_NCBI/bam_files; ./dedup_baq_bams.sh {1} results; rm results/{1}.sort.bam' :::: ../data/Harpur_2014_NCBI/samples.list &> logs/filter_bams_Harpur_2014_NCBI.out &
[2] 11441 - COMPLETE 1.2.19. Deleted intermediate bams to save space (e.g. data/Harpur_2014_NCBI/bam_files/SRR957058.bam)
# would run more in parallel except one other job w/ 2 threads is running at the same time and $ free -m says I have < 6G RAM available, which is what I've maxed picard and samtools to.


# Quick calculation of coverage for pass1 individuals: (~3minutes)
bees/filtered_bams$ angsd -bam ../bee_samples_listed/pass1.bams -doDepth 1 -out results/depth/Group1.1 \
-doCounts 1 -r Group1.1: -minMapQ 30 -minQ 20 -remove_bads 1 -maxDepth 10000
# Note: doesn't appear to count zero-depth spots
# used R to calculate mean depth for this single (large) scaffold to set .5x and 2x approximate cutoffs for angsd GL
plot_bam_metrics.R # (starting with approximation from scaffold Group1.1 only, cutoffs are 865x and 3462x)

# Getting actual depth per chromosome:
# First list scaffolds within each chromosome (and group Un for scaffolds without a chromosome assignment)
bees/data/honeybee_genome$ for i in {1..16} Un; do grep Group$i\. ordered_scaffolds.list > ordered_scaffolds.Group$i.list; done
bees/filtered_bams$ nohup parallel --noswap --joblog logs/depth_by_chr.log --jobs 2 \
'angsd -bam ../bee_samples_listed/pass1.bams -doDepth 1 -out results/depth/Group{1} \
-doCounts 1 -rf ../data/honeybee_genome/ordered_scaffolds.Group{1}.list -minMapQ 30 -minQ 20 \
-remove_bads 1 -maxDepth 20000' ::: {1..16} Un &> logs/depth_by_chr.out &
[1] 13236 - RUNNING 1.7.19

# extracting mapping metrics, including percent read duplication (e.g. PCR duplicates):
# counting # reads pre-filtering, and at de-duplication using PICARD metrics.txt output files
# (to compare effects of filtering across groups/individuals):
bees/filtered_bams/metrics$ grep 'LIBRARY' $(head -n 1 ../../bee_samples_listed/pass1.list).metrics.txt | \
awk '{print "StudyID\t"$0}' > pass1.all.metrics.raw.Nreads; \
for i in $(cat ../../bee_samples_listed/pass1.list); do grep 'Unknown Library' "$i.metrics.txt" | \
awk -v i="$i" '{print i"\t"$0}' >> pass1.all.metrics.raw.Nreads; done

# get the total number of reads, and total # mapped (any quality) AFTER removing duplicates:
# I don't actually filter for Q30 (or even unmapped); wanted to leave options open.
# first count reads using samtools flagstat:
bees/filtered_bams$ nohup parallel --noswap --joblog logs/run_flagstat.log --jobs 4 \
'samtools flagstat results/{1}.sort.dedup.baq.bam > metrics/{1}.flagstat.txt' \
:::: ../bee_samples_listed/pass1.list &> logs/run_flagstat.out &
[2] 15291 - COMPLETE 1.8.19
extract from flagstat metrics the number of reads in final filtered (no duplicates) bams:
bees/filtered_bams/metrics$ for i in $(cat ../../bee_samples_listed/pass1.list); \
do echo $i$'\t'$(cat $i.flagstat.txt | grep 'total' | cut -d' ' -f1); \
done > pass1.all.metrics.dedup.total.Nreads
bees/filtered_bams/metrics$ for i in $(cat ../../bee_samples_listed/pass1.list); \
do echo $i$'\t'$(cat $i.flagstat.txt | grep '+ 0 mapped (' | cut -d' ' -f1); \
done > pass1.all.metrics.dedup.mapped.Nreads

# another way to quickly visualize results from samtools flagstat output and picard's markduplicates is multiQC:
pip3 install multiqc # install
bees/filtered_bams/metrics$ multiqc . # created summary output file: gitErin/bees/filtered_bams/metrics/multiqc_report.html
# from this I see that a few of Ramirez bees, e.g. ap41, and some of my newer bees have very low mapping rates. In general my duplication rate is fine.

# Use FastQ screen to assess why some samples have high % unmapped reads -- do I have contamination? FastQ screen attempts to map reads to multiple references of your choice, to check for contamination
# I downloaded fastq_screen program and multiple reference genomes to ~/Software/fastQ_screen
# made a configuration file including the following genomes for mapping as possible sources of contamination: honeybee, honeybee mtdna, human, PhiX, E. coli (bacteria proxy), Adapter sequences, Arabidopsis (angiosperm proxy for possible pollen contamination), Vectors, plus some additional commonly sequenced things that could contribute to sequencer contamination, lambda, worm, yeast, drosophila, rat, mouse, (human?) mitochondria: fastq_screen_honeybee.conf
# made symlink to fastq_Screen_v0.13.0 folder because needs to have .conf file in that folder:
bees/filtered_bams/fastQ_screen$ ln -s fastq_screen_honeybee.conf ~/Software/fastQ_screen/fastq_screen_v0.13.0/fastq_screen.conf
# ran test (mouse data -- looks good! deleted output after checking)
bees/filtered_bams/fastQ_screen$ ~/Software/fastQ_screen/fastq_screen_v0.13.0/fastq_screen --aligner BOWTIE2 --conf fastq_screen_honeybee.conf ~/Software/fastQ_screen/fastq_screen_test_dataset/fqs_test_dataset.fastq.gz
# just one of my files to test:
bees/filtered_bams/fastQ_screen$ ~/Software/fastQ_screen/fastq_screen_v0.13.0/fastq_screen --aligner BOWTIE2 --conf fastq_screen_honeybee.conf ../../data/novo_seq/C202SC18101772/raw_data/CA0102/CA0102_USPD16090569-AK1956-AK389_HTMNTCCXY_L6_1.fq.gz
# Running fastQ_screen on all my fastq's sequenced thus far:
bees/filtered_bams/fastQ_screen$ nohup parallel --noswap --delay 2 --jobs 4 --joblog ../logs/run_fastQ_screen_C202SC18101772.log '~/Software/fastQ_screen/fastq_screen_v0.13.0/fastq_screen --aligner BOWTIE2 --conf fastq_screen_honeybee.conf --outdir output $(ls ../../data/novo_seq/C202SC18101772/raw_data/{1}/{1}_*_{2}.fq.gz)' :::: ../../data/novo_seq/C202SC18101772/samples.list ::: 1 2 &> ../logs/run_fastQ_screen_C202SC18101772.out &
[1] 28891 - RUNNING 1.19.19
# I summarize these multi-genome mapping results using multiQC:
bees/filtered_bams/fastQ_screen/output$ multiqc .
# this searched for parsable output files (here from fastq_screen) and summarized results in one file: bees/filtered_bams/fastQ_screen/output/multiqc_report.html
# From this I conclude that a small number of samples have a majority or reads that don't map to anything, e.g. AR1115, AR2202 and AR2913.
# These samples could have contamination for which I have no close reference -- they do, e.g., have a little more E. coli mapping than others,
# but I think maybe adapter contamination is worth investigating.
# See TO DO below for next steps to solving this mystery.


# Downloading additional A/C/M/ and new O reference bees in addition to one male haploid drone and several Africanized bees from Brazil from Wallberg 2014
# These require their own bioinformatics pipeline because they were sequenced on an older platform, ABI SOLiD technology, at ~4-6x coverage each
# First get SSR ID's for NCBI download from all bees in Wallberg 2014
data/Wallberg_2014$ (awk -F"\t" '$10 != "USA" && $10 != "Japan" && $10 != "geo_loc_name" {print $7}' SraRunTable.txt; awk -F"\t" '$12 == "male" {print $7}' SraRunTable.txt) > SRR_Acc_to_get.list
data/Wallberg_2014$ (awk -F"\t" '$10 != "USA" && $10 != "Japan" && $10 != "geo_loc_name" {print $8}' SraRunTable.txt; awk -F"\t" '$12 == "male" {print $8}' SraRunTable.txt) > sample_IDs_to_get.list
# Note: many of the bees have multiple files representing different sequencing runs, which creates repeats in sample_IDs_to_get.list
# Download files from NCBI onto Nancy's computer:
data/Wallberg_2014$ nohup parallel --joblog ../../logs/download_Wallberg_2014_NCBI.log --delay 2.5 --noswap 'fastq-dump --split-files -clip \
--gzip --skip-technical -O fastq_files/ "{1}"' :::: SRR_Acc_to_get.list &> ../../logs/download_Wallberg_2014_NCBI.out &
[1] 19643 - oops fastq-dump not installed...I'm installing sratoolkit with homebrew
[2] 40694 - RUNNING 1.19.19

# the next step is to align this SOLiD data using SHRiMP (Rumble et al. 2009). Then I can remove duplicates and adjust by BAQ. And ultimately merge files/runs for the same sample before incorporating these samples into ANGSD variant calling and genotype likelihoods.
# testing my script for downloading aligning and sorting SOLiD data from Wallberg study:
filtered_bams$ chmod u+x download_map_sort_SOLiD_reads.sh
filtered_bams$ nohup parallel --joblog logs/test_download_filter_Wallberg_2014_NCBI.log --noswap './download_map_sort_SOLiD_reads.sh {1} {2}' ::: SRR1151485 ::: SRS549155 &> logs/test_download_filter_Wallberg_2014_NCBI.out &
[1] 72364 - killed, fixed typo in tmp directory, restarting:
[2] 72570 - finished running & all looked good (except see below!). Now modified to delete intermediate files & running whole set:
# ACTUALLY, there was an issue with truncated bam file. I think next step in script is proceeding without finishing first step...need to resolve.
filtered_bams$ nohup parallel --joblog logs/test2_download_filter_Wallberg_2014_NCBI.log --noswap './download_map_sort_SOLiD_reads.sh {1} {2}' ::: SRR1151485 SRR1151486 ::: SRS549155 &> logs/test2_download_filter_Wallberg_2014_NCBI.out &
[1] 91045 - COMPLETED 1.22.19 - ran without errors. I will add to code commands to delete intermediate files. Unclear what issue may have been. Moved test files to filtered_bams/test/SOLiD
# moved results of test to Barbara comp.: filtered_bams/test/SOLiD/SRS549155

# index honeybee genome and save for all calls to SHRiMP 1.22.19:
honeybee_genome$ /Users/ecalfee/Software/SHRiMP_2_2_2/bin/gmapper-cs --save Amel_4.5_scaffolds.fa Amel_4.5_scaffolds.fa
Saving genome map to Amel_4.5_scaffolds.fa # saves .genome and .seed.0 .seed.1 .seed.2 files.
#BUT this doesn't load when I use this genome for SHRiMP, so I deleted these files.

# Run mapping script on all Wallberg 2014 samples 1.22.19:
# note: parallel --link option means each SRR and sample_ID are run together only once (not every possible combination!)
filtered_bams$ nohup parallel --joblog logs/download_filter_Wallberg_2014_NCBI.log --noswap --delay 5s --link './download_map_sort_SOLiD_reads.sh {1} {2}' :::: ../data/Wallberg_2014/SRR_Acc_to_get.list :::: ../data/Wallberg_2014/sample_IDs_to_get.list &> logs/download_filter_Wallberg_2014_NCBI.out &
[1] 22588 - RUNNING 1.23.19 - DID NOT WORK (STALLED OUT)

# making list of just the M/C/O/A reference bees from Wallberg (no admixed Brazil and no high-coverage drone -- can add these in later)
data/Wallberg_2014$ (awk -F"\t" '$10 != "USA" && $10 != "Japan" && $10 != "geo_loc_name" && $10 != "Brazil" {print $7}' SraRunTable.txt) > SRR_Acc_to_get_ACMO.list
data/Wallberg_2014$ (awk -F"\t" '$10 != "USA" && $10 != "Japan" && $10 != "geo_loc_name" && $10 != "Brazil" {print $8}' SraRunTable.txt) > sample_IDs_to_get_ACMO.list

# Kohn data - 2 San Diego bees and 1 Mexico bee sample
# map data (multiple lanes of illumina reads per bee):
nohup parallel --noswap --joblog logs/map_reads_and_sort_kohn_San_Diego.log --jobs 3 \
'./map_reads_kohn.sh SanDiego{1} {2} San_Diego_Honeybee_{1}; \
./sort_reads.sh SanDiego{1} {2}' ::: 001 002 ::: L006 L007 L008 \
&> logs/map_reads_and_sort_kohn_San_Diego.out &
[1] 19236 - Map completed. Sort script was misspelled -- sort_BAMS.sh. Will sort below. 2.1.19

nohup parallel --noswap --joblog logs/map_reads_and_sort_kohn_Mexico.log --jobs 1 \
'./map_reads_kohn.sh Mexico{1} {2} Mexico_Honeybee_{1}; \
./sort_reads.sh Mexico{1} {2}' ::: 001 ::: L006 L007 \
&> logs/map_reads_and_sort_kohn_Mexico.out &
[2] 19347 - COMPLETED (but not sorted, see below)

# After sorting, I merge bams:
# e.g. L006/SanDiego001.sort.bam L007/SanDiego001.sort.bam L008/SanDiego001.sort.bam into merged/SanDiego001.sort.bam
# then run dedup_baq_bams.sh on the resulting merged bam file
nohup parallel --delay 3 --noswap --joblog logs/sort_merge_dedup_kohn.log --jobs 3 \
'./sort_merge_bams_kohn.sh {1}; ./dedup_baq_bams.sh {1} merged' \
::: SanDiego001 SanDiego002 Mexico001 &> logs/sort_merge_dedup_kohn.out &
[2] 29013 - couldn't allocate memory .. so reduced memory. also possibly had lingering jobs running .. oops:
[1] 30349 - RAN but error in merge 2.1.19. Rerunning merge and dedup:
nohup parallel --delay 3 --noswap --joblog logs/merge_dedup_kohn2.log --jobs 3 \
'ls L*/{1}.sort.bam > merged/{1}.list;
samtools merge -b merged/{1}.list merged/{1}.sort.bam;
./dedup_baq_bams.sh {1} merged' ::: SanDiego001 SanDiego002 Mexico001 &> logs/merge_dedup_kohn2.out &
[1] 3391 - RUNNING 2.2.19 and also fixed sort_merge_bams_kohn.sh to reflect these fixes to samtools merge.
# note that Mexico honeybee I'm only using 2/3 (no L008 file) of the data but that's ok coverage for a quick ngsADMIX analysis

# TO DO:
# run all Wallberg 2014 data through pipeline. Make new script to merge bams by sample ID, markduplicates, calculate BAQ and index bams.
# Use fastq_screen to see what the reads look like that don't have any hits -- are they contaminants from something I haven't considered?
# Perhaps more likely, are they on average shorter reads, from libraries where the enzyme cut more and the sequencer read past the sample DNA into the adapter (?)
# I can use fastq_screen with a limit on # reads subsampled to create a small file with a sample of unmapped reads and see what they look like and their lengths.
# If adapter contamination seems likely, I can use something like SeqPurge to trim off adapter sequences. (Sturm et al. 2016, https://doi.org/10.1186/s12859-016-1069-7)
# Download and map reads from Wallberg 2014 for A/C/M/O populations using pipeline for SOLiD data; sort
# merge then deduplicate and run BAQ for Wallberg 2014 bees
# note: best to put more general scripts, that will be used in multiple analyses, when that comes up, in a scripts/ folder
