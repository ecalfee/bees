# Analysis of genotype likelihoods (in ANGSD)
# 2 main SNP sets:
# (1) variant sites called from genotype likelihood analysis in ANGSD and
# (2) variant sites pre-identified from reference populations (SNP set from Julie)

# make list of included bees and corresponding bams for pass1 analysis, including referenced A, M, and C bees, plus any CA bees collected
# after the Africanized honey bee invasion reached CA in 1994, and the first set of sequenced new CA/AR bees:
bees/bee_samples_listed$ cat post_1994.list > pass1.list; cat ../data/novo_seq/C202SC18101772/samples.list >> pass1.list
bees/bee_samples_listed$ for i in $(cat pass1.list); do echo ../filtered_bams/results/$i.sort.dedup.baq.bam; done > pass1.bams

# For calling variant sites in ANGSD, we divide the genome into regions
# and use reference and CA_Bees in addition to first lane of newly sequenced bees, all listed in pass1.list
# We use a non-HW dependent SNP calling algorithm based on counts of reads in ANGSD
# and filter out sites with unusually high or low coverage (> 2x or < .5x mean coverage)
# then the samtools GL function to get genotype likelihoods for each individual,
# all filtering out low quality reads and bases
# for now I just use minimum total depth, not a minimum # individuals cutoff

bees/geno_lik_and_SNPs$ mkdir logs
bees/geno_lik_and_SNPs$ nohup parallel --noswap --joblog logs/pass1_GL_by_scaffold.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/pass1.bams {1} 865 3462 results/pass1_GL_by_scaffold' :::: \
../data/honeybee_genome/ordered_scaffolds.list &> logs/pass1_GL_by_scaffold.out &
[1] 24545 - COMPLETE 1.3.19 (~10hrs)

# turns out I had a duplicated record in pass1.list (ap50). Moved old bam lists to bee_samples_listed/with_duplicated_ap50
# and created new nonduplicated lists for all.meta post_1994.meta post_1994.list and pass1.list, e.g.:
bee_samples_listed$ uniq with_duplicated_ap50/pass1.list > pass1.list

# then added Kohn bees to make new GL and SNP files including these bees:
# note: I did not re-calculate coverage, so these are just estimates for about .5x and 2x cutoffs based on adding 3 bees of ~20x coverage
bee_samples_listed$ cp pass1.list pass1_plus_kohn.list; for i in SanDiego001 SanDiego002 Mexico001; do echo $i >> pass1_plus_kohn.list; done
bee_samples_listed$ for i in $(cat pass1_plus_kohn.list); do echo ../filtered_bams/results/$i.sort.dedup.baq.bam; done > pass1_plus_kohn.bams
geno_lik_and_SNPs$ sleep 2h; nohup parallel --noswap --joblog logs/pass1_plus_kohn_GL_by_scaffold.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/pass1_plus_kohn.bams {1} 900 3600 results/pass1_plus_kohn_GL_by_scaffold' :::: \
../data/honeybee_genome/ordered_scaffolds.list &> logs/pass1_plus_kohn_GL_by_scaffold.out &
[1] 22384 -- ERRORS 2.2.19 - file SRCD12B was empty. re-running after remaking that filtered bam (added sleep 4h to wait for remaking bam):
-- SLEEPING 2 hour delay to send in job (will install 'at' later)
# COMPLETED. but with multiple warnings (because I forgot to copy over index from RG added file.):
# [W::hts_idx_load2] The index file is older than the data file: ../filtered_bams/results/SRCD12B.sort.dedup.baq.bam.bai
# So, saved to pass1_plus_kohn_GL_by_scaffold_backup and rerunning:
geno_lik_and_SNPs$ nohup parallel --noswap --joblog logs/pass1_plus_kohn_GL_by_scaffold_rerun2.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/pass1_plus_kohn.bams {1} 900 3600 results/pass1_plus_kohn_GL_by_scaffold' :::: \
../data/honeybee_genome/ordered_scaffolds.list &> logs/pass1_plus_kohn_GL_by_scaffold_rerun2.out &
[1] 1277 - RUNNING 2.3.19


# Adding in Wallberg 2014 bees:

# first make list of bams:
bee_samples_listed$ cat pass1_plus_kohn.list Wallberg_12O_6M_6C_5A.list > pass1_plus_kohn_and_wallberg.list
bee_samples_listed$ for i in $(cat pass1_plus_kohn_and_wallberg.list); do echo ../filtered_bams/results/$i.sort.dedup.baq.bam; done > pass1_plus_kohn_and_wallberg.bams

# what is an average depth per bee in Wallberg (for low & high depth cutoffs)?
# using flagstat for the first few files I get # reads, multiply by 75bp/read, and for these first files
# see 1.6-4.5x coverage. If I use on average 4x coverage, that's 112x over 28 bees, or very loosely:
+50 for lower (.5x) bound, and + 250 for upper (2x) bound -- I should just recalculate this from flagstat after it runs
geno_lik_and_SNPs$ bedtools genomecov -ibam ../filtered_bams/results/SRS549709.sort.dedup.baq.bam -g ../data/honeybee_genome/ordered_scaffolds.lengths > coverage/SRS549709.coverage
Group10.15      0       51707   243982  0.21193
Group10.15      1*0.246567 + 2*0.218364 + 3*0.155036 + 4*0.0880393 + 5*0.0471879
-- about 1.66 coverage
data/honeybee_genome$ samtools view -H ../../filtered_bams/results/SRS549754.sort.dedup.baq.bam | grep 'SQ' | tr ":" "\t" | cut -f3,5 > genome_order_scaffolds.lengths # used bam header to get correct scaffold order to match genome
data/honeybee_genome$ awk '{print $1"\t"0"\t"$2}' ordered_scaffolds.lengths > ordered_scaffolds.bed
# had to reorder by hand to match genome:
data/honeybee_genome$ awk '{print $1"\t"0"\t"$2}' genome_order_scaffolds.lengths > genome_order_scaffolds.bed

# let's look at coverage for a subset of sites across the genome:
# make a short bed file with 100 random regions 100bp each:
data/honeybee_genome$ bedtools random -g genome_order_scaffolds.lengths -n 100 | sort --version-sort | cut -f1,2,3 > random_100_regions.bed
# reordered by hand to match genome scaffold order (ugh)

# calculate coverage across these regions:
geno_lik_and_SNPs$ bedtools coverage -a ../data/honeybee_genome/random_100_regions.bed -b $(cat ../bee_samples_listed/pass1_plus_kohn_and_wallberg.bams) -sorted -g ../data/honeybee_genome/genome_order_scaffolds.lengths -hist > coverage/pass1_plus_kohn_and_wallberg_random_100_regions.coverage &
[1] 3362 - RUNNING (takes a few hours..)

# use R script to calculate coverage: calc_coverage_from_bedtools_hist.R
# There is about a mean of 2000x coverage in the whole sample (looking at just the first 27 regions)

# running GL scripts with Kohn and Wallberg bees:
geno_lik_and_SNPs$ nohup parallel --noswap --joblog logs/pass1_plus_kohn_and_wallberg_GL_by_scaffold.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/pass1_plus_kohn_and_wallberg.bams {1} 900 4000 results/pass1_plus_kohn_and_wallberg_GL_by_scaffold' :::: \
../data/honeybee_genome/ordered_scaffolds.list &> logs/pass1_plus_kohn_and_wallberg_GL_by_scaffold.out &
[2] 4225 - RUNNING 2.8.19 - COMPLETED


# new sequences in! making SNP set with full set CA/AR bees plus kohn, CA/Mex > 1999 & Ref (will exclude bees not in this study later)

# make list of IDs and bams:
bees/bee_samples_listed$ cat pass1_plus_kohn_and_wallberg.list lanes2_5_IDs.list | sort > CA_AR_MX_harpur_sheppard_kohn_wallberg.list
bees/bee_samples_listed$ awk '{print "../filtered_bams/results/"$0".sort.dedup.baq.bam"}' CA_AR_MX_harpur_sheppard_kohn_wallberg.list > CA_AR_MX_harpur_sheppard_kohn_wallberg.bams

# to calculate coverage, making random list of 100,000 positions on chromosomes in honey bee reference genome
honeybee_genome$ bedtools random -g genome_order_scaffolds.lengths -n 100000 -l 1 | bedtools sort -faidx genome_order_scaffolds.lengths -i stdin > random_pos_100000.bed # not excluding scaffolds with unknown chromosomes
bees/data/honeybee_genome$ grep -v "GroupUn" genome_order_scaffolds.lengths > genome_order_scaffolds_chr_only.lengths # limit to 340 scaffolds on chromosomes
data/honeybee_genome$ bedtools random -g genome_order_scaffolds_chr_only.lengths -n 100000 -l 1 | bedtools sort -faidx genome_order_scaffolds.lengths -i stdin > random_pos_100000_chr_only.bed # get 100,000 positions, sorted
# calculating coverage across those 100,000 positions for reads passing mapping quality 30:
geno_lik_and_SNPs$ mkdir -p results/CA_AR_MX_harpur_sheppard_kohn_wallberg/coverage/
geno_lik_and_SNPs$ bedtools multicov -q 30 -bams $(paste -s -d" " ../bee_samples_listed/CA_AR_MX_harpur_sheppard_kohn_wallberg.bams) -bed ../data/honeybee_genome/random_pos_100000_chr_only.bed > results/CA_AR_MX_harpur_sheppard_kohn_wallberg/coverage/random_pos_100000_chr_only.txt
RUNNING IN FOREGROUND 6.5.19

# make list of just CA/AR IDs and bams 1999-2018 + reference sequences ACM from Harpur and Sheppard
bees/bee_samples_listed$ cat pass1.list lanes2_5_IDs.list | grep -v "MX" | sort > CA_AR_ACM.list
bees/bee_samples_listed$ awk '{print "../filtered_bams/results/"$0".sort.dedup.baq.bam"}' CA_AR_ACM.list > CA_AR_ACM.bams
geno_lik_and_SNPs$ mkdir -p results/CA_AR_ACM/coverage/
geno_lik_and_SNPs$ nohup bedtools multicov -q 30 -bams $(paste -s -d" " ../bee_samples_listed/CA_AR_ACM.bams) -bed ../data/honeybee_genome/random_pos_100000_chr_only.bed > results/CA_AR_ACM/coverage/random_pos_100000_chr_only.txt &
[1] 20794 - RUNNING 6.5.19

# make full set of SNPs - all bees
geno_lik_and_SNPs$ nohup parallel --noswap --joblog logs/CA_AR_MX_harpur_sheppard_kohn_wallberg_GL_by_scaffold.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/CA_AR_MX_harpur_sheppard_kohn_wallberg.bams {1} 1508 6032 results/CA_AR_MX_harpur_sheppard_kohn_wallberg/GL_by_scaffold' :::: \
../data/honeybee_genome/ordered_scaffolds.list &> logs/CA_AR_MX_harpur_sheppard_kohn_wallberg_GL_by_scaffold.out &
[1] 26105 - RUNNING 6.5.19 # mean is 3016x

# not run yet but mean for limited set of bees is 2786x
geno_lik_and_SNPs$ nohup parallel --noswap --joblog logs/CA_AR_ACM_GL_by_scaffold.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/CA_AR_ACM.bams {1} 1393 5572 results/CA_AR_ACM/GL_by_scaffold' :::: \
../data/honeybee_genome/ordered_scaffolds.list &> logs/CA_AR_ACM_GL_by_scaffold.out &
TO RUN LATER



# TO DO

# Call SNPs on HAv3.1 for CA 2014/2018 + AR + ACM
# I will call SNPs for all scaffolds, but run global ancestry 2 ways:
# 1) with just scaffolds localized on the chromosomes
# 2) all scaffolds except mtDNA (so all nuclear scaffolds)
# there is no recombination map for GroupUN, but there are 406 genes on the largest unlocalized scaffold NW_020555859.1
# so these large scaffolds are likely worth including in ancestry inference
# note: I will likely have to recall SNPs on mtDNA for different depth

# generated list of scaffolds and chromosomes in new genome HAv3.1 using subdivide_genome_HAv3.1.R
# note: scaffold information and gap information was downloaded from ncbi: ftp://ftp.ncbi.nih.gov/genomes/Apis_mellifera/ (gaps 9.13.19: allcontig.agp.gz)
# added GroupUn only (all scaffolds except mtDNA and the 16 scaffolded to chromosomes)
data/honeybee_genome$ grep '^NW' scaffolds.list > GroupUN.list
data/honeybee_genome$ cut -f1,2 GroupUN.list > GroupUN.lengths
honeybee_genome$ cut -f3 GroupUN.list > GroupUN.names # also did same to make names files for chr and scaffolds and scaffolds_with_mtDNA

# to calculate coverage, making random list of 100,000 positions on chromosomes in honey bee reference genome HAv3.1. I exclude known seq. gaps. and make sure no positions are repeated.

# random positions, including all scaffolds, even those with unknown chromosomes
data/honeybee_genome$ bedtools random -g scaffolds.lengths -n 100000 -l 1 | bedtools shuffle -i stdin -g scaffolds.lengths -noOverlapping -excl gaps.bed | bedtools sort -faidx scaffolds.lengths -i stdin | cut -f1-3 > scaffolds.random_pos_100000.bed

# random positions only on main scaffold localized on each chromosome
data/honeybee_genome$ bedtools random -g chr.lengths -n 100000 -l 1 | bedtools shuffle -i stdin -g chr.lengths -noOverlapping -excl gaps.bed | bedtools sort -faidx chr.lengths -i stdin | cut -f1-3 > chr.random_pos_100000.bed

# calculating coverage across those 100,000 positions for reads passing mapping quality 30:
geno_lik_and_SNPs$ mkdir -p results/combined_sept19/coverage
geno_lik_and_SNPs$ bedtools multicov -q 30 -bams $(paste -s -d" " ../bee_samples_listed/combined_sept19.bams) -bed ../data/honeybee_genome/chr.random_pos_100000.bed > results/combined_sept19/coverage/chr.random_pos_100000.txt
RUNNING INTERACTIVELY 9.13.19. I ended it early. Far too long.
geno_lik_and_SNPs$ nohup bedtools multicov -q 30 -bams $(paste -s -d" " ../bee_samples_listed/combined_sept19.bams) -bed ../data/honeybee_genome/scaffolds.random_pos_100000.bed > results/combined_sept19/coverage/scaffolds.random_pos_100000.txt &> logs/coverage_scafffolds_combined_sep19_random_pos_100000.out &
[1] 17795 - RUNNING 9.13.19 - I ended it early. Too long run time for what it's worth.

# (!) this is taking far too long. Redo with just 1000 positions to get a coverage estimate:
data/honeybee_genome$ bedtools random -g scaffolds.lengths -n 1000 -l 1 | bedtools shuffle -i stdin -g scaffolds.lengths -noOverlapping -excl gaps.bed | bedtools sort -faidx scaffolds.lengths -i stdin | cut -f1-3 > scaffolds.random_pos_1000.bed
# random positions only on main scaffold localized on each chromosome
data/honeybee_genome$ bedtools random -g chr.lengths -n 1000 -l 1 | bedtools shuffle -i stdin -g chr.lengths -noOverlapping -excl gaps.bed | bedtools sort -faidx chr.lengths -i stdin | cut -f1-3 > chr.random_pos_1000.bed
geno_lik_and_SNPs$ bedtools multicov -q 30 -bams $(paste -s -d" " ../bee_samples_listed/combined_sept19.bams) -bed ../data/honeybee_genome/chr.random_pos_1000.bed > results/combined_sept19/coverage/chr.random_pos_1000.txt
RUNNING INTERACTIVELY 9.13.19 1:20pm
geno_lik_and_SNPs$ bedtools multicov -q 30 -bams $(paste -s -d" " ../bee_samples_listed/combined_sept19.bams) -bed ../data/honeybee_genome/scaffolds.random_pos_1000.bed > results/combined_sept19/coverage/scaffolds.random_pos_1000.txt
RUNNING INTERACTIVELY 9.13.19 1:22pm

# use calc_coverage_from_bedtools_hist.R to find mean coverage and see what % genome is dropped
# using 2x and 0.5x mean cutoffs (or alternatively maybe better to use # individuals w/ data as low cutoff)
# I found that 2x is ~ 5500 depth and 174 is a good minimum number of individuals (=half)

# calling SNPs (all scaffolds):
geno_lik_and_SNPs$ nohup parallel --noswap --joblog logs/call_snps_combined_sept19.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/combined_sept19.bams {1} {2} 174 5500 results/combined_sept19/GL_by_scaffold' \
::: $(cut -f3 ../data/honeybee_genome/scaffolds.list) :::+ \
$(cut -f1 ../data/honeybee_genome/scaffolds.list) &> logs/call_snps_combined_sept19.out &
[1] 30627 - RUNNING 9.14.19 early am

# I first make variant sites files for all SNPs in the dataset (takes just a couple minutes)
geno_lik_and_SNPs$ mkdir -p results/combined_sept19/variant_sites
geno_lik_and_SNPs$ for i in $(cat ../data/honeybee_genome/chr.names); do zcat results/combined_sept19/GL_by_scaffold/${i}.mafs.gz | \
tail -n +2 | awk '{print $1 "\t" $2 "\t" $3 "\t" $4}' \
> results/combined_sept19/variant_sites/${i}.var.sites; sleep 2s; \
angsd sites index results/combined_sept19/variant_sites/${i}.var.sites; done
# make regions file for each sites file (so angsd doesn't walk through the whole genome):
geno_lik_and_SNPs$ for i in $(cat ../data/honeybee_genome/chr.names); do cut -f1 results/combined_sept19/variant_sites/${i}.var.sites | sort | uniq > results/combined_sept19/variant_sites/${i}.regions; done

# I also made a variant sites file for all chr together, but it is so large it's unlikely to be useful:
geno_lik_and_SNPs/results/combined_sept19/variant_sites$ (for i in {1..16}; do cat Group$i.var.sites; done) > chr.var.sites
geno_lik_and_SNPs/results/combined_sept19/variant_sites$ angsd sites index chr.var.sites


# then I use angsd to call genotypes for the high coverage reference panels
# using known major/minor from sites file, no phred quality cutoff (error rates go into model),
# and requiring minimum 6 reads to call a genotype.
# I run A, C and M separately because of the HW assumption.
geno_lik_and_SNPs$ nohup parallel --jobs 4 --noswap --joblog logs/call_genotypes_ACM_combined_sept19.log './call_genotypes.sh {1} combined_sept19 {2}' ::: A C M :::: ../data/honeybee_genome/chr.names &> logs/call_genotypes_ACM_combined_sept19.out &
[1] 6983 - RUNNING 9.16.19


# DENOMINATOR 4 PI:
# I calculate the total number of SNPs that would meet quality cutoffs (>= min Ind with data, <= 2x mean depth, map and base Q)
# -maxDepth 1 just lumps everything with 1+ read (assuming it meets other quality filters) into one column for counting
# output is two files. depthSample counts the bases with 0 or 1+ coverage in each sample (order of bams file) while depthGlobal gives a count for the total combined sample
geno_lik_and_SNPs$ angsd -r NC_037638.1:0-10000 -ref "../data/honeybee_genome/Amel_HAv3.1.fasta" -bam ../bee_samples_listed/combined_sept19.bams -doDepth 1 -out strict4real -doCounts 1 -minMapQ 30 -minQ 20 -minInd 174 -setMaxDepth 5500 -maxDepth 1
# now I need to run this on all 1cM regions to put into my pi bootstrap
# I made 1cM_bins.bed, 1cM_bins.names and 1cM_bins.regions files in the results/ folder using bp_to_r_WallbergHAv3.1.R (interactively)
geno_lik_and_SNPs$ nohup parallel --noswap --jobs 4 --joblog logs/calc_total_coverage_1cM_bins.log 'angsd -r {1} -ref "../data/honeybee_genome/Amel_HAv3.1.fasta" -bam ../bee_samples_listed/combined_sept19.bams -doDepth 1 -out results/coverage_1cM_bins/{2} -doCounts 1 -minMapQ 30 -minQ 20 -minInd 174 -setMaxDepth 5500 -maxDepth 1' :::: results/1cM_bins.regions :::: results/1cM_bins.names &> logs/calc_total_coverage_1cM_bins.out &
[1] 19624 - running 2.28.20. UGH. REDO. NEEDS TO BE LINKED NAMES AND REGIONS!
geno_lik_and_SNPs$ nohup parallel --noswap --jobs 4 --joblog logs/calc_total_coverage_1cM_bins.log 'angsd -r {1} -ref "../data/honeybee_genome/Amel_HAv3.1.fasta" -bam ../bee_samples_listed/combined_sept19.bams -doDepth 1 -out results/coverage_1cM_bins/{2} -doCounts 1 -minMapQ 30 -minQ 20 -minInd 174 -setMaxDepth 5500 -maxDepth 1' :::: results/1cM_bins.regions ::::+ results/1cM_bins.names &> logs/calc_total_coverage_1cM_bins.out &
[1] 7231 - RUNNING
