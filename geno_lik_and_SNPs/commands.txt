# Analysis of genotype likelihoods (in ANGSD)
# 2 main SNP sets:
# (1) variant sites called from genotype likelihood analysis in ANGSD and
# (2) variant sites pre-identified from reference populations (SNP set from Julie)

# make list of included bees and corresponding bams for pass1 analysis, including referenced A, M, and C bees, plus any CA bees collected
# after the Africanized honey bee invasion reached CA in 1994, and the first set of sequenced new CA/AR bees:
bees/bee_samples_listed$ cat post_1994.list > pass1.list; cat ../data/novo_seq/C202SC18101772/samples.list >> pass1.list
bees/bee_samples_listed$ for i in $(cat pass1.list); do echo ../filtered_bams/results/$i.sort.dedup.baq.bam; done > pass1.bams

# For calling variant sites in ANGSD, we divide the genome into regions
# and use reference and CA_Bees in addition to first lane of newly sequenced bees, all listed in pass1.list
# We use a non-HW dependent SNP calling algorithm based on counts of reads in ANGSD
# and filter out sites with unusually high or low coverage (> 2x or < .5x mean coverage)
# then the samtools GL function to get genotype likelihoods for each individual,
# all filtering out low quality reads and bases
# for now I just use minimum total depth, not a minimum # individuals cutoff

bees/geno_lik_and_SNPs$ mkdir logs
bees/geno_lik_and_SNPs$ nohup parallel --noswap --joblog logs/pass1_GL_by_scaffold.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/pass1.bams {1} 865 3462 results/pass1_GL_by_scaffold' :::: \
../data/honeybee_genome/ordered_scaffolds.list &> logs/pass1_GL_by_scaffold.out &
[1] 24545 - COMPLETE 1.3.19 (~10hrs)

# turns out I had a duplicated record in pass1.list (ap50). Moved old bam lists to bee_samples_listed/with_duplicated_ap50
# and created new nonduplicated lists for all.meta post_1994.meta post_1994.list and pass1.list, e.g.:
bee_samples_listed$ uniq with_duplicated_ap50/pass1.list > pass1.list

# then added Kohn bees to make new GL and SNP files including these bees:
# note: I did not re-calculate coverage, so these are just estimates for about .5x and 2x cutoffs based on adding 3 bees of ~20x coverage
bee_samples_listed$ cp pass1.list pass1_plus_kohn.list; for i in SanDiego001 SanDiego002 Mexico001; do echo $i >> pass1_plus_kohn.list; done
bee_samples_listed$ for i in $(cat pass1_plus_kohn.list); do echo ../filtered_bams/results/$i.sort.dedup.baq.bam; done > pass1_plus_kohn.bams
geno_lik_and_SNPs$ sleep 2h; nohup parallel --noswap --joblog logs/pass1_plus_kohn_GL_by_scaffold.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/pass1_plus_kohn.bams {1} 900 3600 results/pass1_plus_kohn_GL_by_scaffold' :::: \
../data/honeybee_genome/ordered_scaffolds.list &> logs/pass1_plus_kohn_GL_by_scaffold.out &
[1] 22384 -- ERRORS 2.2.19 - file SRCD12B was empty. re-running after remaking that filtered bam (added sleep 4h to wait for remaking bam):
-- SLEEPING 2 hour delay to send in job (will install 'at' later)
# COMPLETED. but with multiple warnings (because I forgot to copy over index from RG added file.):
# [W::hts_idx_load2] The index file is older than the data file: ../filtered_bams/results/SRCD12B.sort.dedup.baq.bam.bai
# So, saved to pass1_plus_kohn_GL_by_scaffold_backup and rerunning:
geno_lik_and_SNPs$ nohup parallel --noswap --joblog logs/pass1_plus_kohn_GL_by_scaffold_rerun2.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/pass1_plus_kohn.bams {1} 900 3600 results/pass1_plus_kohn_GL_by_scaffold' :::: \
../data/honeybee_genome/ordered_scaffolds.list &> logs/pass1_plus_kohn_GL_by_scaffold_rerun2.out &
[1] 1277 - RUNNING 2.3.19


# Adding in Wallberg 2014 bees:

# first make list of bams:
bee_samples_listed$ cat pass1_plus_kohn.list Wallberg_12O_6M_6C_5A.list > pass1_plus_kohn_and_wallberg.list
bee_samples_listed$ for i in $(cat pass1_plus_kohn_and_wallberg.list); do echo ../filtered_bams/results/$i.sort.dedup.baq.bam; done > pass1_plus_kohn_and_wallberg.bams

# what is an average depth per bee in Wallberg (for cutoffs)?
# using flagstat for the first few files I get # reads, multiply by 75bp/read, and for these first files
# see 1.6-4.5x coverage. If I use on average 4x coverage, that's 112x over 28 bees, or very loosely:
+50 for lower (.5x) bound, and + 250 for upper (2x) bound -- I should just recalculate this from flagstat after it runs

# TO DO: then run GL script again (maybe change bounds):
geno_lik_and_SNPs$ nohup parallel --noswap --joblog logs/pass1_plus_kohn_and_wallberg_GL_by_scaffold.log --jobs 4 \
'./calc_GL_all_SNPs.sh ../bee_samples_listed/pass1_plus_kohn_and_wallberg.bams {1} 950 3850 results/pass1_plus_kohn_and_wallberg_GL_by_scaffold' :::: \
../data/honeybee_genome/ordered_scaffolds.list &> logs/pass1_plus_kohn_and_wallberg_GL_by_scaffold.out &


# TO DO:
# Re-calculate cutoffs for coverage to exclude SNPs with low coverage and likely mapping errors
